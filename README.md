# üöÄ YOLO PyTorch ‚Üí ONNX Inference Assessment

This project demonstrates a complete, reproducible workflow for:

* Running inference using a **YOLO PyTorch model**
* Converting the model to **ONNX**
* Validating the ONNX model
* Running inference again using **ONNX Runtime**
* Comparing outputs for consistency

The implementation emphasizes **clean environment setup**, **correctness**, **clear logging**, and **AI-assisted coding best practices**, following the provided assessment guidelines.

---

## üìå Overview

**Key objectives covered in this project:**

* ‚úÖ Fresh Python environment setup
* ‚úÖ PyTorch inference using a YOLO `.pt` model
* ‚úÖ ONNX model conversion and validation
* ‚úÖ ONNX Runtime inference
* ‚úÖ Human-readable outputs (console + annotated images)
* ‚úÖ Optional output comparison for consistency

This repository is intentionally **Python-only** to keep the scope focused on model inference and validation.

---

## üß† Technologies Used

* üêç Python 3.x
* üî• PyTorch
* üì¶ Ultralytics YOLO
* üîÅ ONNX
* ‚ö° ONNX Runtime
* üñºÔ∏è OpenCV / Matplotlib
* ü§ñ AI-assisted coding via **Cursor Pro / Windsurf**

---

## üìÇ Project Structure

```
yolo-assessment/
‚îú‚îÄ‚îÄ pytorch_inference/
‚îÇ   ‚îî‚îÄ‚îÄ run_pytorch.py        # PyTorch YOLO inference
‚îú‚îÄ‚îÄ onnx_conversion/
‚îÇ   ‚îî‚îÄ‚îÄ convert_to_onnx.py    # PyTorch ‚Üí ONNX conversion
‚îú‚îÄ‚îÄ onnx_inference/
‚îÇ   ‚îî‚îÄ‚îÄ run_onnx.py           # ONNX Runtime inference
‚îú‚îÄ‚îÄ utils/
‚îÇ   ‚îî‚îÄ‚îÄ visualization.py     # Bounding box utilities
‚îú‚îÄ‚îÄ images/
‚îÇ   ‚îú‚îÄ‚îÄ image1.png
‚îÇ   ‚îú‚îÄ‚îÄ image2.png
‚îÇ   ‚îî‚îÄ‚îÄ image3.png
‚îú‚îÄ‚îÄ outputs/
‚îÇ   ‚îú‚îÄ‚îÄ pytorch/
‚îÇ   ‚îî‚îÄ‚îÄ onnx/
‚îú‚îÄ‚îÄ requirements.txt
‚îî‚îÄ‚îÄ README.md
```

---

## ‚öôÔ∏è Environment Setup

> A clean environment is created from scratch to ensure reproducibility.

```bash
python -m venv env
source env/bin/activate        # Linux / macOS
env\Scripts\activate           # Windows
pip install -r requirements.txt
```

---

## ‚ñ∂Ô∏è PyTorch Inference

* Loads the YOLO `.pt` model
* Runs inference on the provided images
* Outputs:

  * Bounding box coordinates
  * Class labels
  * Confidence scores
  * Annotated images saved to disk

```bash
python pytorch_inference/run_pytorch.py
```

üìå **This step establishes the baseline output before ONNX conversion.**

---

## üîÑ Convert Model to ONNX

* Converts the PyTorch YOLO model to ONNX format
* Uses fixed input dimensions for stability
* Validates the exported ONNX graph

```bash
python onnx_conversion/convert_to_onnx.py
```

‚úîÔ∏è ONNX model validation is performed using `onnx.checker`.

---

## ‚ö° ONNX Runtime Inference

* Loads the converted ONNX model
* Runs inference on the same input images
* Outputs are logged and saved for comparison

```bash
python onnx_inference/run_onnx.py
```

---

## üìä Output Comparison (Optional)

* PyTorch and ONNX predictions are visually compared
* Bounding boxes are overlaid
* Minor numerical differences are expected due to floating-point precision

This step helps demonstrate **functional equivalence** between the two inference pipelines.

---

## ü§ñ AI-Assisted Coding

Throughout the implementation, **Cursor Pro / Windsurf AI** was used to:

* Scaffold scripts quickly
* Validate ONNX export parameters
* Catch common YOLO/ONNX pitfalls
* Review code structure and robustness

AI tools were used intentionally as **engineering assistants**, not as black-box generators.

---

## üé• Video Walkthrough

A full **screen + audio recording** accompanies this project, covering:

* Environment setup
* PyTorch inference
* ONNX conversion
* ONNX Runtime inference
* Output validation and comparison
* Explanation of design decisions and AI tool usage

‚è±Ô∏è Total runtime: under 60 minutes

---

## ‚úÖ Key Takeaways

* Clean, reproducible environment setup is critical
* Always validate PyTorch inference **before** ONNX conversion
* ONNX Runtime provides portable, efficient inference
* AI-assisted tools improve productivity when used deliberately
* Clear logging and explanations matter as much as correct output

---

## üôå Final Notes

This project focuses on **correctness, clarity, and process**, mirroring real-world production workflows for ML inference pipelines.

Thank you for reviewing this submission.

‚Äî **Yamlak**

---